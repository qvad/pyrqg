"""
Performance Edge Cases Grammar for PostgreSQL
Tests query planner limits, memory pressure, statistics edge cases, and performance boundaries
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from pyrqg.dsl.core import Grammar, choice, template, ref, number, maybe
import psycopg2

# Get database schema optimized for performance testing
def get_performance_schema():
    """Fetch tables and indexes suitable for performance testing"""
    try:
        conn = psycopg2.connect("dbname=postgres")
        cur = conn.cursor()
        
        # Get larger tables for performance testing
        cur.execute("""
            SELECT 
                t.tablename,
                t.n_live_tup as row_count,
                array_agg(a.attname) as columns,
                pg_size_pretty(pg_total_relation_size(c.oid)) as size
            FROM pg_stat_user_tables t
            JOIN pg_class c ON c.relname = t.tablename
            JOIN pg_attribute a ON a.attrelid = c.oid AND a.attnum > 0
            WHERE t.schemaname = 'public'
            AND t.tablename NOT LIKE 'table_%'
            GROUP BY t.tablename, t.n_live_tup, c.oid
            ORDER BY t.n_live_tup DESC
            LIMIT 10
        """)
        
        table_stats = {}
        for table, rows, cols, size in cur.fetchall():
            table_stats[table] = {
                'row_count': rows or 0,
                'columns': cols,
                'size': size
            }
        
        # Get indexed columns
        cur.execute("""
            SELECT 
                t.tablename,
                i.indexname,
                array_agg(a.attname) as indexed_columns
            FROM pg_indexes i
            JOIN pg_stat_user_tables t ON i.tablename = t.tablename
            JOIN pg_class c ON c.relname = i.indexname
            JOIN pg_index idx ON idx.indexrelid = c.oid
            JOIN pg_attribute a ON a.attrelid = idx.indrelid AND a.attnum = ANY(idx.indkey)
            WHERE i.schemaname = 'public'
            AND t.tablename NOT LIKE 'table_%'
            GROUP BY t.tablename, i.indexname
            LIMIT 20
        """)
        
        indexed_columns = {}
        for table, index, cols in cur.fetchall():
            if table not in indexed_columns:
                indexed_columns[table] = []
            indexed_columns[table].extend(cols)
        
        cur.close()
        conn.close()
        
        return table_stats, indexed_columns
    except:
        # Fallback
        return (
            {
                "users": {'row_count': 1000, 'columns': ['id', 'name', 'email'], 'size': '8192 bytes'},
                "products": {'row_count': 500, 'columns': ['id', 'name', 'price'], 'size': '8192 bytes'},
                "orders": {'row_count': 2000, 'columns': ['id', 'user_id', 'total'], 'size': '16 kB'}
            },
            {"users": ["id"], "products": ["id"], "orders": ["id", "user_id"]}
        )

table_stats, indexed_columns = get_performance_schema()

# Extract tables sorted by size
tables = list(table_stats.keys())
large_tables = [t for t in tables if table_stats[t]['row_count'] > 100]
small_tables = [t for t in tables if table_stats[t]['row_count'] <= 100]

# Get all columns
all_columns = []
for table, stats in table_stats.items():
    all_columns.extend(stats['columns'])
all_columns = list(set(all_columns))

# Ensure we have tables
if not tables:
    tables = ["users", "products", "orders"]
if not large_tables:
    large_tables = tables
if not small_tables:
    small_tables = tables

g = Grammar("performance_edge_cases")

# Main performance test categories
g.rule("query",
    choice(
        ref("planner_stress_tests"),
        ref("memory_pressure_tests"),
        ref("statistics_edge_cases"),
        ref("join_explosion_tests"),
        ref("sort_and_hash_tests"),
        ref("parallel_query_tests"),
        weights=[20, 20, 15, 15, 20, 10]
    )
)

# Query planner stress tests
g.rule("planner_stress_tests",
    choice(
        # Nested subqueries
        template("""-- Deep subquery nesting
EXPLAIN (ANALYZE, BUFFERS)
SELECT {column}
FROM {table} t1
WHERE {column} IN (
    SELECT {column} FROM {table} t2
    WHERE {column} IN (
        SELECT {column} FROM {table} t3
        WHERE {column} IS NOT NULL
    )
)"""),
        
        # Many OR conditions
        template("""-- OR condition explosion
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM {table}
WHERE {column} = 1
   OR {column} = 2
   OR {column} = 3
   OR {column} = 4
   OR {column} = 5
   OR {column} > 1000"""),
        
        # Complex CASE expressions
        template("""-- Complex CASE in WHERE
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM {table}
WHERE CASE 
    WHEN {column} < 10 THEN {column} * 2
    WHEN {column} < 100 THEN {column} * 3
    WHEN {column} < 1000 THEN {column} * 4
    ELSE {column}
END > 100"""),
        
        # CTE recursion limit
        template("""-- Recursive CTE depth test
WITH RECURSIVE deep_cte AS (
    SELECT 1 as level, {column} FROM {table} WHERE {column} = 1
    UNION ALL
    SELECT level + 1, t.{column}
    FROM {table} t, deep_cte
    WHERE t.{column} = deep_cte.{column} + 1
    AND level < 100
)
SELECT COUNT(*) FROM deep_cte""")
    )
)

# Memory pressure tests
g.rule("memory_pressure_tests",
    choice(
        # Sort memory stress
        template("""-- Sort memory stress test
SET LOCAL work_mem = '64kB';
EXPLAIN (ANALYZE, BUFFERS)
SELECT DISTINCT {column}
FROM {large_table}
ORDER BY {column}"""),
        
        # Hash join memory stress
        template("""-- Hash join memory pressure
SET LOCAL work_mem = '64kB';
EXPLAIN (ANALYZE, BUFFERS)
SELECT COUNT(*)
FROM {large_table} t1
JOIN {large_table} t2 ON t1.{column} = t2.{column}"""),
        
        # Hash aggregate memory
        template("""-- Hash aggregate disk spill
SET LOCAL work_mem = '64kB';
SET LOCAL enable_hashagg = true;
EXPLAIN (ANALYZE, BUFFERS)
SELECT {column}, COUNT(*), SUM({column})
FROM {large_table}
GROUP BY {column}"""),
        
        # Window function memory
        template("""-- Window function memory test
SET LOCAL work_mem = '64kB';
EXPLAIN (ANALYZE, BUFFERS)
SELECT {column},
    ROW_NUMBER() OVER (ORDER BY {column}),
    RANK() OVER (ORDER BY {column}),
    DENSE_RANK() OVER (ORDER BY {column})
FROM {large_table}""")
    )
)

# Statistics edge cases
g.rule("statistics_edge_cases",
    choice(
        # Correlated columns
        template("""-- Correlated column test
EXPLAIN (ANALYZE, BUFFERS)
SELECT * FROM {table}
WHERE {column1} = {column2}
AND {column1} > 0"""),
        
        # Skewed data distribution
        template("""-- Skewed data distribution test
WITH skew_analysis AS (
    SELECT {column},
        COUNT(*) as frequency,
        COUNT(*) * 100.0 / SUM(COUNT(*)) OVER () as percentage
    FROM {table}
    GROUP BY {column}
)
SELECT * FROM skew_analysis
WHERE percentage > 10
ORDER BY frequency DESC"""),
        
        # NULL statistics
        template("""-- NULL value statistics
EXPLAIN (ANALYZE, BUFFERS)
SELECT COUNT(*) FILTER (WHERE {column} IS NULL) as nulls,
       COUNT(*) FILTER (WHERE {column} IS NOT NULL) as non_nulls,
       COUNT(DISTINCT {column}) as distinct_values
FROM {large_table}"""),
        
        # Histogram boundaries
        template("""-- Statistics histogram boundary test
WITH stats AS (
    SELECT 
        MIN({column}) as min_val,
        MAX({column}) as max_val,
        AVG({column})::numeric(10,2) as avg_val,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY {column}) as median
    FROM {table}
)
SELECT * FROM stats""")
    )
)

# Join explosion tests
g.rule("join_explosion_tests",
    choice(
        # Cartesian product warning
        template("""-- Near-cartesian product test
EXPLAIN (ANALYZE, BUFFERS, TIMING OFF)
SELECT COUNT(*)
FROM {table1} t1
CROSS JOIN {table2} t2
WHERE t1.{column} < 10 AND t2.{column} < 10"""),
        
        # Multi-way join
        template("""-- Multi-way join test
EXPLAIN (ANALYZE, BUFFERS)
SELECT COUNT(*)
FROM {table1} t1
JOIN {table2} t2 ON t1.{column} = t2.{column}
JOIN {table3} t3 ON t2.{column} = t3.{column}
WHERE t1.{column} > 0"""),
        
        # Self-join explosion
        template("""-- Self-join multiplication
EXPLAIN (ANALYZE, BUFFERS)
SELECT COUNT(*)
FROM {table} t1
JOIN {table} t2 ON t1.{column} < t2.{column}
WHERE t1.{column} < 100"""),
        
        # Lateral join performance
        template("""-- LATERAL join performance test
EXPLAIN (ANALYZE, BUFFERS)
SELECT t1.{column}, lat.count
FROM {table} t1,
LATERAL (
    SELECT COUNT(*) as count
    FROM {table} t2
    WHERE t2.{column} = t1.{column}
) lat
LIMIT 100""")
    )
)

# Sort and hash tests
g.rule("sort_and_hash_tests",
    choice(
        # Large sort
        template("""-- Large sort operation
EXPLAIN (ANALYZE, BUFFERS)
SELECT {column}
FROM {large_table}
ORDER BY {column}
LIMIT 1000"""),
        
        # Sort with duplicates
        template("""-- Sort with high duplicate ratio
EXPLAIN (ANALYZE, BUFFERS)
SELECT {column}, COUNT(*)
FROM {large_table}
GROUP BY {column}
ORDER BY COUNT(*) DESC, {column}"""),
        
        # Hash distinct
        template("""-- Hash-based DISTINCT
SET LOCAL enable_sort = false;
EXPLAIN (ANALYZE, BUFFERS)
SELECT DISTINCT {column}
FROM {large_table}"""),
        
        # Merge join vs hash join
        template("""-- Force merge join
SET LOCAL enable_hashjoin = false;
SET LOCAL enable_nestloop = false;
EXPLAIN (ANALYZE, BUFFERS)
SELECT *
FROM {table1} t1
JOIN {table2} t2 ON t1.{column} = t2.{column}""")
    )
)

# Parallel query tests
g.rule("parallel_query_tests",
    choice(
        # Parallel sequential scan
        template("""-- Parallel sequential scan
SET LOCAL max_parallel_workers_per_gather = 4;
EXPLAIN (ANALYZE, BUFFERS, VERBOSE)
SELECT COUNT(*) FROM {large_table}
WHERE {column} > 0"""),
        
        # Parallel aggregate
        template("""-- Parallel aggregation
SET LOCAL max_parallel_workers_per_gather = 4;
SET LOCAL parallel_setup_cost = 0;
SET LOCAL parallel_tuple_cost = 0;
EXPLAIN (ANALYZE, BUFFERS)
SELECT {column}, COUNT(*), AVG({column})
FROM {large_table}
GROUP BY {column}"""),
        
        # Parallel join
        template("""-- Parallel hash join
SET LOCAL max_parallel_workers_per_gather = 4;
EXPLAIN (ANALYZE, BUFFERS)
SELECT COUNT(*)
FROM {large_table} t1
JOIN {large_table} t2 ON t1.{column} = t2.{column}"""),
        
        # Parallel safety test
        template("""-- Test parallel safety boundaries
EXPLAIN (VERBOSE)
SELECT *
FROM {table}
WHERE {column} > RANDOM() * 100""")
    )
)

# Table rules
g.rule("table", choice(*tables))
g.rule("large_table", choice(*large_tables))
g.rule("small_table", choice(*small_tables))
g.rule("table1", choice(*tables[:len(tables)//3]) if len(tables) >= 3 else tables[0])
g.rule("table2", choice(*tables[len(tables)//3:2*len(tables)//3]) if len(tables) >= 3 else tables[0])
g.rule("table3", choice(*tables[2*len(tables)//3:]) if len(tables) >= 3 else tables[0])

# Column rules
g.rule("column", choice(*all_columns) if all_columns else "id")
g.rule("column1", choice(*all_columns) if all_columns else "id")
g.rule("column2", choice(*all_columns) if all_columns else "id")

# Indexed columns for better performance
indexed_cols = []
for cols in indexed_columns.values():
    indexed_cols.extend(cols)
if indexed_cols:
    g.rule("indexed_column", choice(*list(set(indexed_cols))))
else:
    g.rule("indexed_column", "id")

# Export grammar
grammar = g